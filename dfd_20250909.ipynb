{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2C1F987pWU9jzv6xm5ZSo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ronie-casaclang/dfd-model/blob/main/dfd_20250909.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Libraries\n",
        "\n",
        "1.   Open CV\n",
        "2.   Pillow\n",
        "3.   TensorFlow\n",
        "4.   MatplotLib\n",
        "5.   Scikit-Learn"
      ],
      "metadata": {
        "id": "vUTxdIk-Q8Hq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Jz2-WuUQqW2"
      },
      "outputs": [],
      "source": [
        "!pip install mtcnn opencv-python pillow tensorflow matplotlib scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "8_GWIQOJRmes"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, cv2\n",
        "from mtcnn import MTCNN\n",
        "from PIL import Image\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "CoA381EFRq58"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Dataset from PC Directory\n",
        "(Video → Cropped Face Frames 299x299)"
      ],
      "metadata": {
        "id": "2RrunMR6Rytb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths (adjust for your machine)\n",
        "input_real = \"C:/Users/Ronie Casaclang/Desktop/dataset/real/\"\n",
        "input_fake = \"C:/Users/Ronie Casaclang/Desktop/dataset/fake/\"\n",
        "output_real = \"C:/Users/Ronie Casaclang/Desktop/frames/real/\"\n",
        "output_fake = \"C:/Users/Ronie Casaclang/Desktop/frames/fake/\"\n",
        "\n",
        "os.makedirs(output_real, exist_ok=True)\n",
        "os.makedirs(output_fake, exist_ok=True)\n",
        "\n",
        "# Initialize detector\n",
        "detector = MTCNN()\n",
        "fps_extract = 5\n",
        "\n",
        "def extract_and_crop_faces(input_path, output_path, fps_extract=5):\n",
        "    for file in os.listdir(input_path):\n",
        "        if file.endswith(\".mp4\"):\n",
        "            vidcap = cv2.VideoCapture(os.path.join(input_path, file))\n",
        "            fps = int(vidcap.get(cv2.CAP_PROP_FPS))\n",
        "            frame_interval = max(1, fps // fps_extract)\n",
        "\n",
        "            success, frame = vidcap.read()\n",
        "            count, saved = 0, 0\n",
        "\n",
        "            while success:\n",
        "                if count % frame_interval == 0:\n",
        "                    faces = detector.detect_faces(frame)\n",
        "                    for i, face in enumerate(faces):\n",
        "                        x, y, w, h = face['box']\n",
        "                        x, y = max(0, x), max(0, y)\n",
        "                        cropped_face = frame[y:y+h, x:x+w]\n",
        "\n",
        "                        if cropped_face.size != 0:\n",
        "                            face_img = Image.fromarray(cv2.cvtColor(cropped_face, cv2.COLOR_BGR2RGB))\n",
        "                            face_img = face_img.resize((299, 299))\n",
        "                            frame_name = f\"{os.path.splitext(file)[0]}_{saved}_{i}.jpg\"\n",
        "                            face_img.save(os.path.join(output_path, frame_name))\n",
        "                            saved += 1\n",
        "\n",
        "                success, frame = vidcap.read()\n",
        "                count += 1\n",
        "\n",
        "            if saved > 0:\n",
        "                print(f\"✅ {file} → {saved} faces saved\")\n",
        "            else:\n",
        "                print(f\"⚠️ {file} → No faces detected, skipped!\")\n",
        "\n",
        "# Run for real and fake datasets\n",
        "extract_and_crop_faces(input_real, output_real, fps_extract)\n",
        "extract_and_crop_faces(input_fake, output_fake, fps_extract)\n"
      ],
      "metadata": {
        "id": "ZiJJnMhTR-0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess & Split\n",
        "\n",
        "\n",
        "\n",
        "1.   Load images from frames/real/ and frames/fake/.\n",
        "2.   Resize & normalize them (you already extracted as 299×299, but we’ll double-check).\n",
        "3.   Label them (0 = real, 1 = fake).\n",
        "4.   Split into Train / Validation / Test (70/15/15).\n",
        "5.   Convert into NumPy arrays ready for training.\n",
        "\n"
      ],
      "metadata": {
        "id": "vl3Zz1cmSR9P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Define base path\n",
        "base_dir = \"C:/Users/Ronie Casaclang/Desktop/frames/\"\n",
        "\n",
        "# Common parameters\n",
        "img_size = (299, 299)   # matches InceptionV3/Xception expected input\n",
        "batch_size = 32\n",
        "\n",
        "# Load datasets directly from folders\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    base_dir + \"train\",\n",
        "    labels=\"inferred\",         # auto-detect \"real\" and \"fake\"\n",
        "    label_mode=\"categorical\",  # one-hot encoding for multi-class (2 classes)\n",
        "    image_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    seed=42\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    base_dir + \"val\",\n",
        "    labels=\"inferred\",\n",
        "    label_mode=\"categorical\",\n",
        "    image_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "test_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    base_dir + \"test\",\n",
        "    labels=\"inferred\",\n",
        "    label_mode=\"categorical\",\n",
        "    image_size=img_size,\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Optimize performance with caching + prefetching\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
        "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "# Check class names (should print [\"fake\", \"real\"] or vice versa)\n",
        "print(\"Class names:\", train_ds.class_names)\n"
      ],
      "metadata": {
        "id": "UCqXrUogSXKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What this does\n",
        "\n",
        "*   Reads images from your train/, val/, and test/ folders.\n",
        "*   Automatically labels images based on subfolder name (real = class 0, fake = class 1, or vice versa).\n",
        "*   Resizes every image to 299×299 (good for Xception/InceptionV3).\n",
        "*   Converts labels into one-hot vectors ([1,0] = real, [0,1] = fake).\n",
        "*   Adds caching + prefetching for faster training.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "y3EYkVlrTJ8r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Augmentation\n",
        "\n",
        "What this does\n",
        "*   Data Augmentation = generating “new” training images by slightly modifying existing ones → helps the model generalize better and not just memorize.\n",
        "\n",
        "Why it matters for Deepfake detection:\n",
        "*   Faces can appear at slightly different angles, brightness, zoom, etc.\n",
        "*   Augmentation teaches the model to recognize “real vs fake” even when images are not perfectly aligned.\n",
        "\n",
        "\n",
        "How it connects to the workflow\n",
        "*   This augmentation layer is applied only to training data (not validation/test).\n",
        "*   Later in Cell 6 (Build model), you’ll put this layer at the start of your CNN model (or apply it directly on batches from train_ds)."
      ],
      "metadata": {
        "id": "yYk4Cyq-Takn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Define data augmentation pipeline\n",
        "data_augmentation = keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),   # flip left-right\n",
        "    layers.RandomRotation(0.1),        # rotate image ±10%\n",
        "    layers.RandomZoom(0.1),            # zoom in/out 10%\n",
        "    layers.RandomContrast(0.1),        # adjust contrast ±10%\n",
        "], name=\"data_augmentation\")\n",
        "\n",
        "# Example: apply augmentation to one batch\n",
        "for images, labels in train_ds.take(1):\n",
        "    augmented_images = data_augmentation(images)\n",
        "    print(\"Original batch shape:\", images.shape)\n",
        "    print(\"Augmented batch shape:\", augmented_images.shape)\n"
      ],
      "metadata": {
        "id": "qlUGpTYiTtMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build Model\n",
        "\n",
        "### What this does\n",
        "*   Pretrained on ImageNet, already knows rich features (edges, textures, shapes).\n",
        "*   include_top=False → remove default classifier, so we can add our own real/fake classifier.\n",
        "*   trainable=False → freeze backbone first (faster training, avoids overfitting). Later you can fine-tune.\n",
        "\n",
        "### Why it matters for Deepfake detection:\n",
        "*   GlobalAveragePooling2D() → reduces feature maps into a single vector per image.\n",
        "*   Dropout(0.5) → randomly drops neurons, prevents overfitting.\n",
        "*   Dense(2, softmax) → final classifier with 2 outputs (real/fake).\n",
        "\n",
        "\n",
        "### Loss/metrics:\n",
        "*   categorical_crossentropy since labels are one-hot encoded ([1,0] vs [0,1]).\n",
        "*   accuracy to track performance."
      ],
      "metadata": {
        "id": "PvjkHGSfUl60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.applications import Xception\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Input shape (must match augmentation + dataset)\n",
        "input_shape = (299, 299, 3)\n",
        "\n",
        "# Base model: Xception pretrained on ImageNet\n",
        "base_model = Xception(\n",
        "    weights=\"imagenet\",\n",
        "    include_top=False,        # exclude the final classification layer\n",
        "    input_shape=input_shape\n",
        ")\n",
        "base_model.trainable = False  # freeze weights (fine-tune later if needed)\n",
        "\n",
        "# Build model with augmentation + base + custom classifier\n",
        "inputs = layers.Input(shape=input_shape)\n",
        "x = data_augmentation(inputs)        # apply augmentation from Cell 5\n",
        "x = base_model(x, training=False)    # forward pass through frozen backbone\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dropout(0.5)(x)           # regularization\n",
        "outputs = layers.Dense(2, activation=\"softmax\")(x)  # 2 classes: real/fake\n",
        "\n",
        "model = models.Model(inputs, outputs)\n",
        "\n",
        "# Compile model\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Model summary\n",
        "model.summary()\n"
      ],
      "metadata": {
        "id": "mFp19RSBVQZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the Model\n",
        "\n",
        "What this does\n",
        "*   model.fit(): trains the model using batches from train_ds, validates on val_ds after each epoch.\n",
        "*   model.fit(): trains the model using batches from train_ds, validates on val_ds after each epoch.\n",
        "*   EarlyStopping: stops training if validation loss doesn’t improve for 5 epochs (avoids overfitting).\n",
        "*   ModelCheckpoint: saves the best model (deepfake_xception_best.h5) based on validation loss."
      ],
      "metadata": {
        "id": "3U3_XoqfVhF8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "# Callbacks\n",
        "early_stop = EarlyStopping(\n",
        "    monitor=\"val_loss\",\n",
        "    patience=5,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "checkpoint = ModelCheckpoint(\n",
        "    \"deepfake_xception_best.h5\",\n",
        "    monitor=\"val_loss\",\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Training\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=20,                 # start small (10–20), can increase if needed\n",
        "    callbacks=[early_stop, checkpoint],\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "id": "R-M7s6I2V1k5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize Training Results\n",
        "\n",
        "What this does\n",
        "*   Extracts accuracy and loss values from the history object (Cell 7).\n",
        "\n",
        "Create 2 plots:\n",
        "*   Accuracy plot: shows how well the model learns to classify real vs fake.\n",
        "*   Loss plot: shows whether the model is converging or overfitting."
      ],
      "metadata": {
        "id": "K6_tS-5NWDCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Get training history\n",
        "acc = history.history[\"accuracy\"]\n",
        "val_acc = history.history[\"val_accuracy\"]\n",
        "loss = history.history[\"loss\"]\n",
        "val_loss = history.history[\"val_loss\"]\n",
        "\n",
        "epochs_range = range(1, len(acc) + 1)\n",
        "\n",
        "# Plot Accuracy\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label=\"Training Accuracy\")\n",
        "plt.plot(epochs_range, val_acc, label=\"Validation Accuracy\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.title(\"Training vs Validation Accuracy\")\n",
        "\n",
        "# Plot Loss\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label=\"Training Loss\")\n",
        "plt.plot(epochs_range, val_loss, label=\"Validation Loss\")\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.title(\"Training vs Validation Loss\")\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ba299RtdWOXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate on Test Data\n",
        "\n",
        "What this does\n",
        "*   Runs the trained model on the test dataset (completely unseen images).\n",
        "\n",
        "Reports:\n",
        "*   Test Accuracy → how well the model generalizes to new data.\n",
        "*   Test Loss → measure of prediction error."
      ],
      "metadata": {
        "id": "h1GYWIPVWR7W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the trained model on the unseen test set\n",
        "test_loss, test_acc = model.evaluate(test_ds, verbose=2)\n",
        "\n",
        "print(\"\\n✅ Test Accuracy:\", round(test_acc * 100, 2), \"%\")\n",
        "print(\"✅ Test Loss:\", round(test_loss, 4))\n"
      ],
      "metadata": {
        "id": "eTU4QvQ5WlJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export Model for Django\n",
        "\n",
        "1.   List itemSavedModel format → recommended for serving inside TensorFlow Serving or directly loading in Django.\n",
        "2.   H5 format → widely compatible, you can load_model(\"deepfake_model.h5\") easily in Django views.\n",
        "3.   TFLite format → optimized for mobile/edge, useful if you later extend your project to Android/iOS apps\n",
        "\n",
        "\n",
        "\n",
        "What this does\n",
        "*   Runs the trained model on the test dataset (completely unseen images).\n",
        "\n",
        "Reports:\n",
        "*   Test Accuracy → how well the model generalizes to new data.\n",
        "*   Test Loss → measure of prediction error."
      ],
      "metadata": {
        "id": "IxxiQAiMWndM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# 1. Save the model in TensorFlow SavedModel format (good for serving)\n",
        "saved_model_dir = \"exported_model/saved_model\"\n",
        "model.save(saved_model_dir)\n",
        "\n",
        "print(\"✅ Model saved in TensorFlow SavedModel format at:\", saved_model_dir)\n",
        "\n",
        "# 2. Save as HDF5 (.h5) format (optional, widely supported in Keras/Django integrations)\n",
        "h5_model_path = \"exported_model/deepfake_model.h5\"\n",
        "model.save(h5_model_path)\n",
        "\n",
        "print(\"✅ Model also saved as H5 file at:\", h5_model_path)\n",
        "\n",
        "# 3. Convert to TensorFlow Lite (.tflite) for mobile/edge deployment\n",
        "import tensorflow as tf\n",
        "\n",
        "tflite_model_path = \"exported_model/deepfake_model.tflite\"\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "with open(tflite_model_path, \"wb\") as f:\n",
        "    f.write(tflite_model)\n",
        "\n",
        "print(\"✅ Model converted to TFLite at:\", tflite_model_path)\n"
      ],
      "metadata": {
        "id": "H8AjMBeyWq-V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}